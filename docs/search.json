{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n",
      "last_modified": "2021-12-08T21:09:57-07:00"
    },
    {
      "path": "CV.html",
      "title": "Resume",
      "description": "My Resume.\n",
      "author": [
        {
          "name": "Henry Caleb Brown",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\nHENRY CALEB BROWN\r\n149 Ora Lane | Caledonia. MS 39740 | (662)313-6942 | Brown.caleb1717@yahoo.com\r\nOBJECTIVE\r\nSeeking an Accounting/Auditing position utilizing the following background:\r\nManaging a lifeguard staff of over 15 people\r\nProviding exceptional customer service in the sales department\r\nManaging the clothing department and supervising sales\r\nOver 60 hours of Accounting and Accounting related classes\r\nEDUCATION\r\nBachelor in Business Administration Mississippi State University, Starkville, MS\r\nGraduated: December 2019\r\nMajor: Business Administration\r\nMinor: Accounting\r\nOverall GPA: 3.36\r\nProgram GPA: 3.47\r\nMasters in Professional Accountancy, Mississippi State University, Starkville, MS\r\nEstimated Graduation Date: Spring 2022\r\nMajor: Professional Accountancy\r\nMinor: Accounting Data Analytics\r\nCurrent GPA: 3.66\r\nSKILLS & ABILITIES\r\nManagement\r\nDelegated tasks to all of staff and worked them hand in hand to run the pool\r\nInfluencing, leading, and delegating abilities\r\nCritical thinking, decision making and problem solving skill\r\nAbility to influence people and clients\r\nPlanning and organizing – Organizational abilities\r\nAdaptability – Efficient under pressure, always meet deadline\r\nSales\r\nCustomer service focus\r\nInterpersonal Skills\r\nKnow how to negotiate prices with customers as well as recommending promotions\r\nExcellent written and verbal communication skills\r\nQuickly develop relationships with client\r\nSound expertise in sales\r\nEXPERIENCE\r\nOfficer-in-Charge (OIC) of Casual Lieutenants United States Air Force, Creech AFB, 432 OSS\r\nMarch ’21 - Present\r\nManage schedules for up to 90 lieutenants at any given time\r\nDelegate tasks to trainees\r\nIn-process new lieutenants arriving to Creech AFB\r\nWork directly with flight commander and squadron commander\r\nSupervising Lifeguard\r\nJoe Frank Sanderson Center, Starkville, MS\r\nMay ’16- August ’19\r\nMade schedules for 15 or more lifeguards\r\nCommunicated with our director and relayed messages to staff\r\nHandled all pool cleaning duties and assigned shifts to others\r\nManaged all scheduling of swim lessons and events coming up\r\nSales Associate\r\nDicks Sporting Goods, Columbus, MS\r\nMay ’16 – September ’16\r\nPrepared merchandise for sales floor\r\nPrioritized and accomplished a wide range of tasks each shift\r\nProvided customer service to any individual interested in buying clothes\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-05T16:04:56-06:00"
    },
    {
      "path": "Final-Project.html",
      "title": "Final Project",
      "description": "Final Project for Acc-8143/Accounting Data Analytics\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\nghdfhgdfghfdhgdfhgfd\r\n\r\n\r\n\r\n",
      "last_modified": "2021-12-07T22:20:03-07:00"
    },
    {
      "path": "Final.html",
      "title": "Final Project",
      "description": "Final Project for Acc-8143/Accounting Data Analytics\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\nClassification\r\n\r\n\r\nknitr::opts_chunk$set(echo = TRUE)\r\n\r\n\r\n\r\n\r\n\r\nMovies<-read.csv(file= 'Movies.csv')\r\n\r\n\r\n\r\n\r\n\r\nlibrary(caret)\r\nlibrary(tidyverse)\r\nset.seed(1)\r\n\r\n\r\n\r\n\r\n\r\n#lets split the data 60/40\r\nlibrary(caret)\r\ntrainIndex <- createDataPartition(iris$Species, p = .6, list = FALSE, times = 1)\r\n\r\n#look at the first few\r\nhead(trainIndex)\r\n\r\n\r\n     Resample1\r\n[1,]         1\r\n[2,]         3\r\n[3,]         4\r\n[4,]         6\r\n[5,]         7\r\n[6,]         9\r\n\r\n\r\n\r\nMoviesTrain <- Movies[ trainIndex,]\r\nMoviesTest  <- Movies[-trainIndex,]\r\n\r\n\r\n\r\n\r\n\r\npreProcValues <- preProcess(MoviesTest, method = c(\"center\", \"scale\"))\r\ntestTransformed <- predict(preProcValues, MoviesTest)\r\n\r\n\r\n\r\n\r\n\r\npsych::describe(MoviesTrain)\r\n\r\n\r\n                  vars  n         mean           sd      median\r\nï..Release.Date*     1 90        45.09        25.70        45.5\r\nMovie.Title*         2 90        45.50        26.12        45.5\r\nProduction.Budget    3 90 204911111.11  42608656.73 200000000.0\r\nDomestic.Gross       4 90 252811463.14 176427848.32 215674372.0\r\nWorldwide.Gross      5 90 735916204.96 498772831.26 653970038.5\r\n                       trimmed          mad       min        max\r\nï..Release.Date*         45.11        32.62         1         89\r\nMovie.Title*             45.50        33.36         1         90\r\nProduction.Budget 197125000.00  34841100.00 165000000  400000000\r\nDomestic.Gross    227392998.00 136471814.78         0  936662225\r\nWorldwide.Gross   667212729.15 406027570.66  69965374 2845899541\r\n                       range  skew kurtosis          se\r\nï..Release.Date*          88 -0.01    -1.22        2.71\r\nMovie.Title*              89  0.00    -1.24        2.75\r\nProduction.Budget  235000000  2.12     5.46  4491346.78\r\nDomestic.Gross     936662225  1.56     2.88 18597128.11\r\nWorldwide.Gross   2775934167  1.88     5.06 52575272.73\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-12-08T21:07:43-07:00"
    },
    {
      "path": "index.html",
      "title": "Henry Brown",
      "author": [],
      "contents": "\r\n\r\n          \r\n          \r\n          Henry Caleb Brown\r\n          \r\n          \r\n          Home\r\n          Resume\r\n          \r\n          \r\n          Projects\r\n           \r\n          ▾\r\n          \r\n          \r\n          R-Squared\r\n          ML\r\n          Final Project\r\n          \r\n          \r\n          \r\n          \r\n          \r\n          ☰\r\n          \r\n          \r\n      \r\n        \r\n          \r\n            \r\n              \r\n            \r\n              Henry Brown\r\n            \r\n            \r\n              \r\n                \r\n                    \r\n                      \r\n                        LinkedIn\r\n                      \r\n                    \r\n                  \r\n                                    \r\n                    \r\n                      \r\n                        Work Email\r\n                      \r\n                    \r\n                  \r\n                                    \r\n                    \r\n                      \r\n                        GitHub\r\n                      \r\n                    \r\n                  \r\n                                    \r\n                    \r\n                      \r\n                        Personal Email\r\n                      \r\n                    \r\n                  \r\n                                  \r\n            \r\n          \r\n        \r\n        \r\n        \r\n          \r\n            I am a 2d Lt in the United States Air Force working as a program manager for the PARCS division. Currently living in Colorado Springs, CO and working at Peterson Air Force Base. I lived in Caledonia, MS for 24 years of my life and went to Mississippi State to get my undergraduate degree in Business Administration. I am currently getting my masters in Professional Accountancy.\r\n          \r\n        \r\n      \r\n    \r\n\r\n    \r\n      \r\n        \r\n          \r\n            \r\n              \r\n            \r\n              Henry Brown\r\n            \r\n            \r\n              \r\n                \r\n                                    \r\n                    \r\n                      LinkedIn\r\n                    \r\n                  \r\n                                    \r\n                    \r\n                      Work Email\r\n                    \r\n                  \r\n                                    \r\n                    \r\n                      GitHub\r\n                    \r\n                  \r\n                                    \r\n                    \r\n                      Personal Email\r\n                    \r\n                  \r\n                                  \r\n              \r\n            \r\n            \r\n              I am a 2d Lt in the United States Air Force working as a program manager for the PARCS division. Currently living in Colorado Springs, CO and working at Peterson Air Force Base. I lived in Caledonia, MS for 24 years of my life and went to Mississippi State to get my undergraduate degree in Business Administration. I am currently getting my masters in Professional Accountancy.\r\n            \r\n        \r\n      \r\n    \r\n\r\n    \r\n    \r\n    ",
      "last_modified": "2021-12-08T21:09:57-07:00"
    },
    {
      "path": "ML.html",
      "title": "ML",
      "description": "This exercise shows off how Machine Learning is used. \n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\nDimensionality Reduction\r\nExplanation Using this machine learning technique can help you cut down on the different dimensions that must be processed. This technique brings out relationships between the original dimensions to identify new dimensions that better capture the relationships within the data.\r\nProcess and Results The graph show that PC1 and PC 2 had the most variances.\r\nHow it applies to Accounting It can help identify the most important features for a data set which will cut down the work that the accountant has to do.\r\n\r\n\r\nlibrary(caret)\r\n#store our data in another object\r\ndat <- iris\r\n#take the 4 continuous variables and perform PCA\r\ncaret.pca <- preProcess(dat[,-5], method=\"pca\",pcaComp=2)\r\n\r\ncaret.pca\r\n\r\n\r\nCreated from 150 samples and 4 variables\r\n\r\nPre-processing:\r\n  - centered (4)\r\n  - ignored (0)\r\n  - principal component signal extraction (4)\r\n  - scaled (4)\r\n\r\nPCA used 2 components as specified\r\n\r\n\r\n\r\ncaret.pca$\r\n#use that data to form our new inputs\r\ndat2 <- predict(caret.pca, dat[,-5])\r\n\r\n\r\n#using stats\r\nstat.pca <- prcomp(dat[,-5],\r\n                 center = TRUE,\r\n                 scale. = TRUE) \r\n\r\n# plot method\r\nplot(stat.pca, type = \"l\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nsummary(stat.pca)\r\n\r\n\r\nImportance of components:\r\n                          PC1    PC2     PC3     PC4\r\nStandard deviation     1.7084 0.9560 0.38309 0.14393\r\nProportion of Variance 0.7296 0.2285 0.03669 0.00518\r\nCumulative Proportion  0.7296 0.9581 0.99482 1.00000\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-12-08T18:50:06-07:00"
    },
    {
      "path": "R-Squared.html",
      "title": "R-Squared",
      "description": "This tab will contain at least one problem with using Rsquared as a model fit measure.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\nTime to blow RSquared up1 💥\r\nR-squared is a statistic that often accompanies regression output. It ranges in value from 0 to 1 and is usually interpreted as summarizing the percent of variation in the response that the regression model explains. So an R-squared of 0.65 might mean that the model explains about 65% of the variation in our dependent variable. Given this logic, we prefer our regression models have a high R-squared.\r\nIn R, we typically get R-squared by calling the summary function on a model object. Here’s a quick example using simulated data:\r\n\r\n\r\n# independent variable\r\nx <- 1:20 \r\n# for reproducibility\r\nset.seed(1) \r\n# dependent variable; function of x with random error\r\ny <- 2 + 0.5*x + rnorm(20,0,3) \r\n# simple linear regression\r\nmod <- lm(y~x)\r\n# request just the r-squared value\r\nsummary(mod)$r.squared          \r\n\r\n\r\n[1] 0.6026682\r\n\r\nOne way to express R-squared is as the sum of squared fitted-value deviations divided by the sum of squared original-value deviations:\r\n\\[\r\nR^{2} =  \\frac{\\sum (\\hat{y} – \\bar{\\hat{y}})^{2}}{\\sum (y – \\bar{y})^{2}}\r\n\\]\r\nWe can calculate it directly using our model object like so:\r\n\r\n\r\n# extract fitted (or predicted) values from model\r\nf <- mod$fitted.values\r\n# sum of squared fitted-value deviations\r\nmss <- sum((f - mean(f))^2)\r\n# sum of squared original-value deviations\r\ntss <- sum((y - mean(y))^2)\r\n# r-squared\r\nmss/tss                      \r\n\r\n\r\n[1] 0.6026682\r\n\r\n1. R-squared does not measure goodness of fit. It can be arbitrarily low when the model is completely correct. By making\\(σ^2\\) large, we drive R-squared towards 0, even when every assumption of the simple linear regression model is correct in every particular.\r\nWhat is \\(σ^2\\)? When we perform linear regression, we assume our model almost predicts our dependent variable. The difference between “almost” and “exact” is assumed to be a draw from a Normal distribution with mean 0 and some variance we call \\(σ^2\\).\r\nThis statement is easy enough to demonstrate. The way we do it here is to create a function that (1) generates data meeting the assumptions of simple linear regression (independent observations, normally distributed errors with constant variance), (2) fits a simple linear model to the data, and (3) reports the R-squared. Notice the only parameter for sake of simplicity is sigma. We then “apply” this function to a series of increasing \\(σ\\) values and plot the results.\r\n\r\n\r\nr2.0 <- function(sig){\r\n  # our predictor\r\n  x <- seq(1,10,length.out = 100)   \r\n  # our response; a function of x plus some random noise\r\n  y <- 2 + 1.2*x + rnorm(100,0,sd = sig) \r\n  # print the R-squared value\r\n  summary(lm(y ~ x))$r.squared          \r\n}\r\nsigmas <- seq(0.5,20,length.out = 20)\r\n # apply our function to a series of sigma values\r\nrout <- sapply(sigmas, r2.0)            \r\nplot(rout ~ sigmas, type=\"b\")\r\n\r\n\r\n\r\n\r\nR-squared tanks hard with increasing sigma, even though the model is completely correct in every respect.\r\nR-squared can be arbitrarily close to 1 when the model is totally wrong.\r\nThe point being made is that R-squared does not measure goodness of fit.\r\n\r\n\r\nset.seed(1)\r\n# our predictor is data from an exponential distribution\r\nx <- rexp(50,rate=0.005)\r\n# non-linear data generation\r\ny <- (x-1)^2 * runif(50, min=0.8, max=1.2) \r\n# clearly non-linear\r\nplot(x,y)             \r\n\r\n\r\n\r\n\r\n\r\n\r\nsummary(lm(y ~ x))$r.squared\r\n\r\n\r\n[1] 0.8485146\r\n\r\nIt’s very high at about 0.85, but the model is completely wrong. Using R-squared to justify the “goodness” of our model in this instance would be a mistake. Hopefully one would plot the data first and recognize that a simple linear regression in this case would be inappropriate.\r\n3. R-squared says nothing about prediction error, even with \\(σ^2\\) exactly the same, and no change in the coefficients. R-squared can be anywhere between 0 and 1 just by changing the range of X. We’re better off using Mean Square Error (MSE) as a measure of prediction error.\r\nMSE is basically the fitted y values minus the observed y values, squared, then summed, and then divided by the number of observations.\r\nLet’s demonstrate this statement by first generating data that meets all simple linear regression assumptions and then regressing y on x to assess both R-squared and MSE.\r\n\r\n\r\nx <- seq(1,10,length.out = 100)\r\nset.seed(1)\r\ny <- 2 + 1.2*x + rnorm(100,0,sd = 0.9)\r\nmod1 <- lm(y ~ x)\r\nsummary(mod1)$r.squared\r\n\r\n\r\n[1] 0.9383379\r\n\r\n# Mean squared error\r\nsum((fitted(mod1) - y)^2)/100\r\n\r\n\r\n[1] 0.6468052\r\n\r\nNow repeat the above code, but this time with a different range of x. Leave everything else the same:\r\n\r\n\r\n # new range of x\r\nx <- seq(1,2,length.out = 100)      \r\nset.seed(1)\r\ny <- 2 + 1.2*x + rnorm(100,0,sd = 0.9)\r\nmod1 <- lm(y ~ x)\r\nsummary(mod1)$r.squared\r\n\r\n\r\n[1] 0.1502448\r\n\r\n# Mean squared error\r\nsum((fitted(mod1) - y)^2)/100        \r\n\r\n\r\n[1] 0.6468052\r\n\r\nThe R-squared falls from 0.94 to 0.15 but the MSE remains the same. In other words the predictive ability is the same for both data sets, but the R-squared would lead you to believe the first example somehow had a model with more predictive power.\r\nR-squared can easily go down when the model assumptions are better fulfilled.\r\nLet’s examine this by generating data that would benefit from transformation. Notice the R code below is very much like our previous efforts but now we exponentiate our y variable.\r\n\r\n\r\nx <- seq(1,2,length.out = 100)\r\nset.seed(1)\r\ny <- exp(-2 - 0.09*x + rnorm(100,0,sd = 2.5))\r\nsummary(lm(y ~ x))$r.squared\r\n\r\n\r\n[1] 0.003281718\r\n\r\nplot(lm(y ~ x), which=3)\r\n\r\n\r\n\r\n\r\nR-squared is very low and our residuals vs. fitted plot reveals outliers and non-constant variance. A common fix for this is to log transform the data. Let’s try that and see what happens:\r\n\r\n\r\nplot(lm(log(y)~x),which = 3) \r\n\r\n\r\n\r\n\r\nThe diagnostic plot looks much better. Our assumption of constant variance appears to be met. But look at the R-squared:\r\n\r\n\r\nsummary(lm(log(y)~x))$r.squared \r\n\r\n\r\n[1] 0.0006921086\r\n\r\nIt’s even lower! This is an extreme case and it doesn’t always happen like this. In fact, a log transformation will usually produce an increase in R-squared. But as just demonstrated, assumptions that are better fulfilled don’t always lead to higher R-squared.\r\nIt is very common to say that R-squared is “the fraction of variance explained” by the regression. \\[Yet\\] if we regressed X on Y, we’d get exactly the same R-squared. This in itself should be enough to show that a high R-squared says nothing about explaining one variable by another.\r\nThis is the easiest statement to demonstrate:\r\n\r\n\r\nx <- seq(1,10,length.out = 100)\r\ny <- 2 + 1.2*x + rnorm(100,0,sd = 2)\r\nsummary(lm(y ~ x))$r.squared\r\n\r\n\r\n[1] 0.737738\r\n\r\nsummary(lm(x ~ y))$r.squared\r\n\r\n\r\n[1] 0.737738\r\n\r\nDoes x explain y, or does y explain x? Are we saying “explain” to dance around the word “cause”? In a simple scenario with two variables such as this, R-squared is simply the square of the correlation between x and y:\r\n\r\n\r\nall.equal(cor(x,y)^2, summary(lm(x ~ y))$r.squared, summary(lm(y ~ x))$r.squared)\r\n\r\n\r\n[1] TRUE\r\n\r\nLet’s recap:\r\nR-squared does not measure goodness of fit.\r\nR-squared does not measure predictive error.\r\nR-squared does not necessarily increase when assumptions are better satisfied.\r\nR-squared does not measure how one variable explains another.\r\n\r\nhttps://data.library.virginia.edu/is-r-squared-useless/↩︎\r\n",
      "last_modified": "2021-12-07T22:14:14-07:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
